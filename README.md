# NKU-COSC0015-人工智能导论
提供实验报告与代码，本学期共有六个实验，难度较大，有几个实验根本不知道怎么写

第一个实验，比较简单，就是一个简单的逻辑分析问题，学习一下kanren的用法就可以轻松写出

第二个实验，自己写出来性能很差，最后勉强加了一个科技算法才把性能拉满

第三个实验，挺简单的，但是给分就不知道了

第四个实验，最逆天的一集，必须使用ppt上的主成分分析来做才能拿到100，不然就是及格分

第五个实验，在线测试100分，但是助教测的又是另一回事了(

第六个实验，收官之战，虽然但是也没多少时间写这玩意了，随便写了写就交了(


## Lab1 斑马问题
**斑马问题**： 5 个不同国家（英国、西班牙、日本、意大利、挪威）且工作各不相同（油漆工、摄影师、外交官、小提琴家、医生）的人分别住在一条街上的 5 所房子里，  每所房子的颜色不同（红色、白色、蓝色、黄色、绿色），每个人都有自己养的不同宠物（狗、蜗牛、斑马、马、狐狸），喜欢喝不同的饮料（矿泉水、牛奶、茶、橘子汁、咖啡）。  
根据以下提示，你能告诉我哪所房子里的人养斑马，哪所房子里的人喜欢喝矿泉水吗？

1. 英国人住在红色的房子里
2. 西班牙人养了一条狗
3. 日本人是一个油漆工
4. 意大利人喜欢喝茶
5. 挪威人住在左边的第一个房子里
6. 绿房子在白房子的右边
7. 摄影师养了一只蜗牛
8. 外交官住在黄房子里
9. 中间那个房子的人喜欢喝牛奶
10. 喜欢喝咖啡的人住在绿房子里
11. 挪威人住在蓝色的房子旁边
12. 小提琴家喜欢喝橘子汁
13. 养狐狸的人所住的房子与医生的房子相邻
14. 养马的人所住的房子与外交官的房子相邻

## Lab2 黑白棋
>  黑白棋问题：黑白棋 (Reversi)，也叫苹果棋，翻转棋，是一个经典的策略性游戏。  
>
>  一般棋子双面为黑白两色，故称“黑白棋”。因为行棋之时将对方棋子翻转，则变为己方棋子，故又称“翻转棋” (Reversi) 。 
>  棋子双面为红、绿色的称为“苹果棋”。它使用 8x8 的棋盘，由两人执黑子和白子轮流下棋，最后子多方为胜方。 
>
>  **游戏规则：**
>
>  1. 黑方先行，双方交替下棋。
>
>  2. 一步合法的棋步包括：
>
>   + 在一个空格处落下一个棋子，并且翻转对手一个或多个棋子；
>   + 新落下的棋子必须落在可夹住对方棋子的位置上，对方被夹住的所有棋子都要翻转过来，  
>     可以是横着夹，竖着夹，或是斜着夹。夹住的位置上必须全部是对手的棋子，不能有空格；  
>   + 一步棋可以在数个（横向，纵向，对角线）方向上翻棋，任何被夹住的棋子都必须被翻转过来，棋手无权选择不去翻某个棋子。  
>
>  3. 如果一方没有合法棋步，也就是说不管他下到哪里，都不能至少翻转对手的一个棋子，那他这一轮只能弃权，而由他的对手继续落子直到他有合法棋步可下。
>  4. 如果一方至少有一步合法棋步可下，他就必须落子，不得弃权。  
>  5. 棋局持续下去，直到棋盘填满或者双方都无合法棋步可下。  
>  6. 如果某一方落子时间超过 1 分钟 或者 连续落子 3 次不合法，则判该方失败。  
>
>  **实验要求：**
>
>  + 使用 **『蒙特卡洛树搜索算法』** 实现 `miniAlphaGo for Reversi`。   
>  + 使用 Python 语言。
>  + 算法部分需要自己实现，不要使用现成的包、工具或者接口。

> **对问题的理解**  
>
> ​       实验平台已经给出了`game.py`和`board.py`的源代码，所以我们理论上只需要实现`AIplayer`模块的代码就可以了。

## Lab3 垃圾短信识别
**1.1 实验背景**
垃圾短信 ($Spam Messages，SM$) 是指未经过用户同意向用户发送不愿接收的商业广告或者不符合法律规范的短信。    
随着手机的普及，垃圾短信在日常生活日益泛滥，已经严重的影响到了人们的正常生活娱乐，乃至社会的稳定。     
据 360 公司 2020 年第一季度有关手机安全的报告提到，360 手机卫士在第一季度共拦截各类垃圾短信约 34.4 亿条，平均每日拦截垃圾短信约 3784.7 万条。      
大数据时代的到来使得大量个人信息数据得以沉淀和积累，但是庞大的数据量缺乏有效的整理规范；   
在面对量级如此巨大的短信数据时，为了保证更良好的用户体验，如何从数据中挖掘出更多有意义的信息为人们免受垃圾短信骚扰成为当前亟待解决的问题。

**1.2 实验要求**

1) 任务提供包括数据读取、基础模型、模型训练等基本代码  
2) 参赛选手需完成核心模型构建代码，并尽可能将模型调到最佳状态  
3) 模型单次推理时间不超过 10 秒 

**1.3 实验环境** 

可以使用基于 $Python$ 的 $Pandas、Numpy、Sklearn$ 等库进行相关特征处理，使用 $Sklearn $框架训练分类器，也可编写深度学习模型，使用过程中请注意 $Python$ 包（库）的版本。

## Lab4 特征人脸识别
**1.1  实验背景**

本实验采用特征脸（$Eigenface$）算法进行人脸识别。

特征脸（$eigenface$）是第一种有效的人脸识别方法，通过在一大组描述不同人脸的图像上进行主成分分析（$PCA$）获得。
本次实验要求大家构建一个自己的人脸库（建议）：大家可以选择基于$ORL$人脸库添加自己搜集到的人脸图像形成一个更大的人脸库，要求人脸库中的每一张图像都只包含一张人脸且眼睛的中心位置对齐(通过裁剪或缩放，使得每张人脸图像大小尺寸一致且人脸眼睛的中心位置对齐)。为了方便同学们操作，大家也可以选择直接基于$ORL$人脸库进行本次实验。

**1.2  实验内容**

在模型训练过程中，首先要根据测试数据求出平均脸，然后将前 $K$ 个特征脸保存下来，利用这$ K$ 个特征脸对测试人脸进行识别，此外对于任意给定的一张人脸图像，可以使用这$K$个特征脸对原图进行重建。

**1.3 实验要求**

1. 求解人脸图像的特征值与特征向量构建特征脸模型
2. 利用特征脸模型进行人脸识别和重建，比较使用不同数量特征脸的识别与重建效果
3. 使用 $Python$ 语言

## Lab5 口罩佩戴检测
**1.1 实验背景**

今年一场席卷全球的新型冠状病毒给人们带来了沉重的生命财产的损失。  
有效防御这种传染病毒的方法就是积极佩戴口罩。  
我国对此也采取了严肃的措施，在公共场合要求人们必须佩戴口罩。  
在本次实验中，我们要建立一个目标检测的模型，可以识别图中的人是否佩戴了口罩。

**1.2 实验要求**

1）建立深度学习模型，检测出图中的人是否佩戴了口罩，并将其尽可能调整到最佳状态。  
2）学习经典的模型 MTCNN 和 MobileNet 的结构。  
3）学习训练时的方法。 

**1.3 实验环境**

可以使用基于 Python 的 OpenCV 、PIL 库进行图像相关处理，使用 Numpy 库进行相关数值运算，使用 Pytorch 等深度学习框架训练模型等。

**1.4 注意事项**
+ Python 与 Python Package 的使用方式，可在右侧 `API文档` 中查阅。
+ 当右上角的『Python 3』长时间指示为运行中的时候，造成代码无法执行时，可以重新启动 Kernel 解决（左上角『Kernel』-『Restart Kernel』）。

## Lab6 机器人自动走迷宫
**1.1 实验背景**

在本实验中，要求分别使用基础搜索算法和$Deep ~QLearning$算法，完成机器人自动走迷宫。

![img](https://imgbed.momodel.cn/20201105105220.png)

如上图所示，左上角的红色椭圆既是起点也是机器人的初始位置，右下角的绿色方块是出口。
游戏规则为:从起点开始，通过错综复杂的迷宫，到达目标点(出口)。

在任一位置可执行动作包括：向上走 `'u'`、向右走 `'r'`、向下走 `'d'`、向左走 `'l'`。

- 执行不同的动作后，根据不同的情况会获得不同的奖励，具体而言，有以下几种情况。
  - 撞墙
  - 走到出口
  - 其余情况
- 你需要实现基于基础搜索算法和$Deep ~QLearning$算法的机器人，使机器人自动走到迷宫的出口。

**1.2 实验要求**

- 使用$Python$语言。
- 使用基础搜索算法完成机器人走迷宫。
- 使用$Deep ~QLearning$算法完成机器人走迷宫。
- 算法部分需要自己实现，不能使用现成的包、工具或者接口。

**1.3 实验环境**

可以使用$Python$实现基础算法的实现， 使用$Keras、PyTorch$等框架实现$Deep~ QLearning$算法。

**1.4 注意事项**

- $Python$与$Python ~Package$的使用方式，可在右侧 $API$文档中查阅。
- 当右上角的『$Python~ 3$』长时间指示为运行中的时候，造成代码无法执行时，可以重新启动$Kernel$解决（左上角『$Kernel$』-『$Restart~ Kernel$』）。

**1.5 参考资料**

- 强化学习入门$MDP$：https://zhuanlan.zhihu.com/p/25498081
- $QLearning$简单例子（英文）：http://mnemstudio.org/path-finding-q-learning-tutorial.htm
- $QLearning$简单解释（知乎）：https://www.zhihu.com/question/26408259
- $DeepQLearning$论文：[https://files.momodel.cn/Playing%20Atari%20with%20Deep%20Reinforcement%20Learning.pdf](https://files.momodel.cn/Playing Atari with Deep Reinforcement Learning.pdf)

